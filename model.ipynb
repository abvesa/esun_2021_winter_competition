{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from brc_pytorch.layers import BistableRecurrentCell, NeuromodulatedBistableRecurrentCell\n",
    "from brc_pytorch.layers import MultiLayerBase\n",
    "\n",
    "from pytorch_lamb import Lamb\n",
    "from adahessian import *\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/all_transaction_v4.pkl', 'rb') as f:\n",
    "    all_transaction = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu(num):\n",
    "    with torch.cuda.device(f'cuda:{num}'):\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_k(batch_proba, batch_label, batch_isnan, topk=3):\n",
    "    '''\n",
    "    batch_proba = (b, m, 16)\n",
    "    batch_label = (b, m, 16)\n",
    "    batch_isnan = (b, m)\n",
    "    '''\n",
    "    batch_proba = batch_proba.cpu()\n",
    "    batch_label = batch_label.cpu()\n",
    "    batch_isnan = batch_isnan.cpu()\n",
    "    \n",
    "    # get topk indexes\n",
    "    batch_pred_idx = batch_proba.argsort(dim=-1, descending=True)[:, :, :topk]\n",
    "    batch_real_idx = batch_label.argsort(dim=-1, descending=True)[:, :, :topk]\n",
    "    \n",
    "    # get num of dcg and idcg with indexes\n",
    "    bs = batch_proba.shape[0]\n",
    "    ms = batch_proba.shape[1]\n",
    "    batch_indexes = torch.repeat_interleave(torch.repeat_interleave(torch.arange(bs).unsqueeze(-1).unsqueeze(-1), topk, dim=-1), ms, dim=1)\n",
    "    month_indexes = torch.repeat_interleave(torch.repeat_interleave(torch.arange(ms).unsqueeze(-1).unsqueeze(0), topk, dim=-1), bs, dim=0)\n",
    "    \n",
    "    batch_pred_amt = batch_label[batch_indexes, month_indexes, batch_pred_idx]\n",
    "    batch_real_amt = batch_label[batch_indexes, month_indexes, batch_real_idx]\n",
    "\n",
    "    # compute ndcg in shape (batch(user), month)\n",
    "    denum = torch.log2((torch.arange(1, topk+1) + 1.0)).unsqueeze(0).unsqueeze(0)\n",
    "    denum = torch.repeat_interleave(torch.repeat_interleave(denum, ms, dim=1), bs, dim=0)\n",
    "    bm_dcg = (batch_pred_amt / denum).sum(dim=-1)    \n",
    "    bm_idcg = (batch_real_amt / denum).sum(dim=-1)\n",
    "    bm_ndcg = bm_dcg / bm_idcg\n",
    "    \n",
    "    # there will be nan months, because user have transactions not in 16 categories\n",
    "    nan_month = torch.isnan(bm_ndcg)\n",
    "    \n",
    "    # replace ndcg of months with no transaction with 0\n",
    "    not_avail_months = batch_isnan + nan_month\n",
    "    bm_ndcg[not_avail_months] = 0.0\n",
    "    \n",
    "    #bm_ndcg[batch_isnan] = 0.0\n",
    "    #bm_ndcg[nan_month] = 0.0\n",
    "    \n",
    "    # compute each user ndcg average by month, result in (b, )\n",
    "    #avail_months = (batch_isnan != True).sum(dim=-1) - nan_month.sum(dim=-1)\n",
    "    avail_months = (~not_avail_months).sum(dim=-1)\n",
    "    avail_months_mask = (avail_months == 0.0)\n",
    "    avail_months[avail_months_mask] = 1.0\n",
    "    b_ndcg = bm_ndcg.sum(dim=-1) / avail_months\n",
    "\n",
    "    return b_ndcg[~avail_months_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loader, topk=3):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = []\n",
    "    ndcgs = []\n",
    "    total_batches = len(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_iter, (ids, xs, cs, ys) in enumerate(loader):\n",
    "            batch_proba, batch_label, batch_isnan, loss = model(xs, cs, ys)            \n",
    "            b_ndcg = ndcg_k(batch_proba, batch_label, batch_isnan, topk=topk)\n",
    "            total_loss.append(loss.item())\n",
    "            ndcgs.extend(b_ndcg.tolist())\n",
    "                \n",
    "            #with torch.cuda.device(f'{model.device.type}:{model.device.index}'):\n",
    "            #    torch.cuda.empty_cache()\n",
    "        \n",
    "    avg_loss = np.mean(total_loss)\n",
    "    avg_ndcg = np.mean(ndcgs)\n",
    "    \n",
    "    print('********* evaluation *********')\n",
    "    print(f'avg loss: {avg_loss}')\n",
    "    print(f'avg ndcg: {avg_ndcg}')\n",
    "    print('******************************')\n",
    "    \n",
    "    return avg_loss, avg_ndcg\n",
    "\n",
    "\n",
    "def train(model, opts, ids, xs, cs, ys):\n",
    "    for opt in opts:\n",
    "        opt.zero_grad()\n",
    "    _, _, _, loss = model(xs, cs, ys)\n",
    "    loss.backward()\n",
    "    \n",
    "    for opt in opts:\n",
    "        opt.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_iter(model, train_dl, valid_dl, opts, print_ratio=0.1):\n",
    "    start = time.time()\n",
    "    train_dl.dataset.change_state('all_train', shuffle=True)\n",
    "    #train_dl.dataset.change_state('train', shuffle=True)\n",
    "    #train_dl.dataset.change_state('valid', shuffle=True)\n",
    "    n_iters = len(train_dl)\n",
    "\n",
    "    print_every = int(n_iters*print_ratio)\n",
    "    if print_every == 0:\n",
    "        print_every = 1\n",
    "        \n",
    "    print_loss_total = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_iter, (ids, xs, cs, ys) in enumerate(train_dl, 1):\n",
    "        loss = train(model, opts, ids, xs, cs, ys)\n",
    "        print_loss_total += loss\n",
    "        \n",
    "        if (batch_iter+1) % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f'{timeSince(start, batch_iter/n_iters)} ({int(batch_iter)}, {int(batch_iter/n_iters * 100)}%) loss:{print_loss_avg:.4f}')\n",
    "        \n",
    "        #with torch.cuda.device(f'{model.device.type}:{model.device.index}'):\n",
    "        #    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('> train')\n",
    "    valid_dl.dataset.change_state('all_train', shuffle=False)\n",
    "    #valid_dl.dataset.change_state('train', shuffle=False)\n",
    "    #valid_dl.dataset.change_state('valid', shuffle=False)\n",
    "    _ = evaluation(model, valid_dl)\n",
    "   \n",
    "    #print('> valid')\n",
    "    #valid_dl.dataset.change_state('all_valid', shuffle=False)\n",
    "    #valid_dl.dataset.change_state('valid', shuffle=False)\n",
    "    #valid_dl.dataset.change_state('train', shuffle=False)\n",
    "    #_ = evaluation(model, valid_dl)\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        scheduler.step(valid_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, batch_size=256, shuffle=False, drop_last=False):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, \n",
    "                                       drop_last=drop_last, collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionDataset():\n",
    "    def __init__(self, fill_last_transaction=False):\n",
    "        self.train_valid_split_index = 19\n",
    "        self.max_seq_len = 24\n",
    "        self.all_data = self.load_data()\n",
    "        self.fold_indexes = self.split_cross_validation()\n",
    "        self.fill_last_transaction = fill_last_transaction\n",
    "        self.curr_fold = 0\n",
    "        self.state = 'train'\n",
    "    \n",
    "    def change_fold(self, k):\n",
    "        assert 0 <= k < 5, 'fold out of range'\n",
    "        self.curr_fold = k\n",
    "    \n",
    "    def change_state(self, state, shuffle=False):\n",
    "        self.state = state\n",
    "        if shuffle:\n",
    "            random.shuffle(self.fold_indexes[self.curr_fold][state])\n",
    "        \n",
    "    def load_data(self, ):\n",
    "        global all_transaction\n",
    "        \n",
    "        all_data = []\n",
    "        for i, (user_id, user_transaction) in enumerate(all_transaction.items()):\n",
    "            if i % 10000 == 0:\n",
    "                print('training_data: ', i)\n",
    "            \n",
    "            user_x = np.zeros((24, 49, 2))\n",
    "            user_c = np.zeros((24, 15))\n",
    "            #user_c = np.zeros((24, 49, 15))\n",
    "            for m in user_transaction:\n",
    "                for t in m:                    \n",
    "                    user_id, month, shop_tag, count, amount, \\\n",
    "                    card_1, card_2, card_3, card_4, card_5, \\\n",
    "                    card_6, card_7, card_8, card_9, card_10, \\\n",
    "                    card_11, card_12, card_13, card_14, other_card = t\n",
    "                    \n",
    "                    count = abs(count)\n",
    "                    month = int(month)-1\n",
    "                    shop_tag_idx = (49 if shop_tag == 'other' else int(shop_tag))-1\n",
    "                    user_x[month][shop_tag_idx][0] += amount\n",
    "                    user_x[month][shop_tag_idx][1] += count\n",
    "                    #user_c[month][shop_tag_idx] = [card_1, card_2, card_3, card_4, card_5, card_6, card_7, card_8, card_9, card_10,\n",
    "                    #                               card_11, card_12, card_13, card_14, other_card]\n",
    "                    \n",
    "            user_y = user_x[1:, :, 0]\n",
    "            all_data.append((user_id, user_x, user_c, user_y))\n",
    "            \n",
    "        # shuffle only once, and rerandomize\n",
    "        random.seed(0)\n",
    "        random.shuffle(all_data)\n",
    "        random.seed()\n",
    "        \n",
    "        return all_data \n",
    "    \n",
    "    def split_cross_validation(self, ):\n",
    "        kf = KFold(n_splits=5, shuffle=False)\n",
    "        fold_indexes = {}\n",
    "        for curr_fold, (tr_idx, va_idx) in enumerate(kf.split(np.arange(len(self.all_data)))):\n",
    "            fold_indexes[curr_fold] = {\n",
    "                'all_train': np.arange(len(self.all_data)).tolist(),\n",
    "                'all_valid': np.arange(len(self.all_data)).tolist(),\n",
    "                'train': tr_idx.tolist(),\n",
    "                'valid': va_idx.tolist(),\n",
    "                'test': np.arange(len(self.all_data)).tolist()\n",
    "            }\n",
    "        return fold_indexes\n",
    "        \n",
    "    def __len__(self, ):    \n",
    "        return len(self.fold_indexes[self.curr_fold][self.state])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        curr_index = self.fold_indexes[self.curr_fold][self.state][idx]\n",
    "        curr_id, curr_x, curr_c, curr_y = self.all_data[curr_index]\n",
    "        if self.state != 'test':\n",
    "            curr_x = curr_x[:-1, :, :]\n",
    "            curr_c = curr_c[:-1, :]\n",
    "            \n",
    "            '''\n",
    "            if 'train' in self.state:\n",
    "                curr_x = curr_x[:self.train_valid_split_index, :, :]\n",
    "                curr_c = curr_c[:self.train_valid_split_index, :]\n",
    "                curr_y = curr_y[:self.train_valid_split_index, :]\n",
    "                \n",
    "            elif 'valid' in self.state:\n",
    "                curr_y = curr_y[self.train_valid_split_index:, :]\n",
    "            '''\n",
    "            \n",
    "        return curr_id, curr_x, curr_c, curr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRecommend(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_layers=1, fillna=False, amount_only=False, topk_loss=3, device=torch.device('cpu')):\n",
    "        super(RNNRecommend, self).__init__()\n",
    "        self.device = device        \n",
    "        self.full_item_size = 49\n",
    "        self.input_size = (49 if amount_only else 49*2)# + 15\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.real_item = torch.tensor([2, 6, 10, 12, 13, 15, 18, 19, 21, 22, 25, 26, 36, 37, 39, 48]) - 1\n",
    "        self.amount_only = amount_only\n",
    "        self.topk = topk_loss\n",
    "        \n",
    "        self.rnn_input_size = self.input_size\n",
    "        self.rnn_layer = nn.GRU(input_size=self.rnn_input_size, hidden_size=hidden_dim, \n",
    "                                num_layers=num_layers, batch_first=True)       \n",
    "        \n",
    "        self.linear_layer = nn.Sequential(\n",
    "                                nn.Linear(hidden_dim, hidden_dim//4),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_dim//4, self.full_item_size),\n",
    "                                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "    \n",
    "    def calc_cos_loss(self, x, y):\n",
    "        cossim = 1.0-torch.cosine_similarity(x.softmax(dim=-1), y, dim=-1)\n",
    "        return cossim.mean()\n",
    "    \n",
    "    def calc_bce_loss(self, x, y):\n",
    "        if self.topk > 0:\n",
    "            bs = y.shape[0]\n",
    "            ms = y.shape[1]\n",
    "            bi = torch.repeat_interleave(torch.repeat_interleave(torch.arange(bs).unsqueeze(-1).unsqueeze(-1), self.topk, dim=-1), ms, dim=1)\n",
    "            mi = torch.repeat_interleave(torch.repeat_interleave(torch.arange(ms).unsqueeze(-1).unsqueeze(0), self.topk, dim=-1), bs, dim=0)\n",
    "            topki = y.topk(self.topk, dim=-1)[1]\n",
    "            y = y[bi, mi, topki]\n",
    "            x = x[bi, mi, topki]\n",
    "        \n",
    "        loss = self.loss_fn(x, y)\n",
    "#         y /= (y.sum(-1).unsqueeze(-1))\n",
    "#         y[torch.isnan(y)] = 1/16\n",
    "#         loss = (-(y * x.log_softmax(dim=-1)).sum(-1)).mean()        \n",
    "        return loss\n",
    "        \n",
    "    def calc_rank_loss(self, x, y, margin=1.0):\n",
    "        paired_x = (x.unsqueeze(3) - x.unsqueeze(2))\n",
    "        paired_y = (y.unsqueeze(3) - y.unsqueeze(2))\n",
    "        paired_y = (paired_y > 0)*1.0\n",
    "        paired_y[paired_y == 0] = -1.0\n",
    "        return F.relu(-paired_y*paired_x+margin).mean()\n",
    "    \n",
    "    def data_preprocess(self, x, y=None, c=None):\n",
    "        px = (x / x.sum(dim=2).unsqueeze(2)).float().to(self.device)\n",
    "        px[torch.isnan(px)] = 1/49\n",
    "\n",
    "        if self.amount_only:\n",
    "            px = px[:, :, :, 0]\n",
    "        else:\n",
    "            px = px.reshape(px.shape[0], px.shape[1], -1)\n",
    "        result = (px, )\n",
    "        \n",
    "        if c is not None:\n",
    "            pc = (c / c.sum(dim=-1).unsqueeze(-1)).float().to(self.device)\n",
    "            pc[torch.isnan(pc)] = 1/15\n",
    "            result += (pc, )\n",
    "        \n",
    "        if y is not None:\n",
    "            py = (y / y.sum(dim=2).unsqueeze(2)).float().to(self.device) # (b, m, 49)\n",
    "            py_nan = torch.isnan(py.sum(dim=-2).unsqueeze(-2))\n",
    "            py_isnan = torch.isnan(py.sum(dim=-1))\n",
    "            py[torch.isnan(py)] = 1/49\n",
    "            result += (py, py_isnan, )\n",
    "        return result\n",
    "    \n",
    "    def forward(self, x, c, y):\n",
    "        px, py, py_isnan = self.data_preprocess(x, y=y)\n",
    "        #px = torch.cat((px, c.float().to(self.device)), dim=-1)\n",
    "        \n",
    "        # model\n",
    "        rnn_output, _ = self.rnn_layer(px)\n",
    "        linear_output = self.linear_layer(rnn_output)\n",
    "        \n",
    "        # calc loss\n",
    "        bce_loss = self.calc_bce_loss(linear_output[:, -py.shape[1]:, self.real_item], py[:, :, self.real_item])\n",
    "        #rank_loss = self.calc_rank_loss(linear_output[:, :, self.real_item], py[:, :, self.real_item], margin=1.0)\n",
    "        #loss = bce_loss# + rank_loss\n",
    "        #loss = self.calc_cos_loss(linear_output[:, -py.shape[1]:, self.real_item], py[:, :, self.real_item])\n",
    "        \n",
    "        return linear_output[:, -py.shape[1]:, self.real_item], py[:, :, self.real_item], py_isnan, bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:  0\n",
      "training_data:  10000\n",
      "training_data:  20000\n",
      "training_data:  30000\n",
      "training_data:  40000\n",
      "training_data:  50000\n",
      "training_data:  60000\n",
      "training_data:  70000\n",
      "training_data:  80000\n",
      "training_data:  90000\n",
      "training_data:  100000\n",
      "training_data:  110000\n",
      "training_data:  120000\n",
      "training_data:  130000\n",
      "training_data:  140000\n",
      "training_data:  150000\n",
      "training_data:  160000\n",
      "training_data:  170000\n",
      "training_data:  180000\n",
      "training_data:  190000\n",
      "training_data:  200000\n",
      "training_data:  210000\n",
      "training_data:  220000\n",
      "training_data:  230000\n",
      "training_data:  240000\n",
      "training_data:  250000\n",
      "training_data:  260000\n",
      "training_data:  270000\n",
      "training_data:  280000\n",
      "training_data:  290000\n",
      "training_data:  300000\n",
      "training_data:  310000\n",
      "training_data:  320000\n",
      "training_data:  330000\n",
      "training_data:  340000\n",
      "training_data:  350000\n",
      "training_data:  360000\n",
      "training_data:  370000\n",
      "training_data:  380000\n",
      "training_data:  390000\n",
      "training_data:  400000\n",
      "training_data:  410000\n",
      "training_data:  420000\n",
      "training_data:  430000\n",
      "training_data:  440000\n",
      "training_data:  450000\n",
      "training_data:  460000\n",
      "training_data:  470000\n",
      "training_data:  480000\n",
      "training_data:  490000\n"
     ]
    }
   ],
   "source": [
    "full_dataset = TransactionDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_dataset.train_valid_split_index = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = get_dataloader(full_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
    "valid_dl = get_dataloader(full_dataset, batch_size=1024, shuffle=False, drop_last=False)\n",
    "test_dl = get_dataloader(full_dataset, batch_size=1024, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNRecommend(\n",
       "  (rnn_layer): GRU(98, 512, batch_first=True)\n",
       "  (linear_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=49, bias=True)\n",
       "    (3): Softmax(dim=-1)\n",
       "  )\n",
       "  (loss_fn): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clear_gpu(1)\n",
    "avail_device = torch.device('cpu')#torch.device('cuda:1')#\n",
    "model = RNNRecommend(hidden_dim=512, num_layers=1, fillna=True, amount_only=False, \n",
    "                     topk_loss=-1, device=avail_device).to(avail_device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_fold = 0\n",
    "train_dl.dataset.change_fold(curr_fold)\n",
    "valid_dl.dataset.change_fold(curr_fold)\n",
    "test_dl.dataset.change_fold(curr_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== epoch 1 =====\n",
      "0m 3s (- 0m 35s) (194, 9%) loss:2.2974\n",
      "0m 7s (- 0m 30s) (389, 19%) loss:2.1969\n",
      "0m 11s (- 0m 27s) (584, 29%) loss:2.1793\n",
      "0m 15s (- 0m 23s) (779, 39%) loss:2.1784\n",
      "0m 19s (- 0m 19s) (974, 49%) loss:2.1760\n",
      "0m 23s (- 0m 15s) (1169, 59%) loss:2.1682\n",
      "0m 27s (- 0m 11s) (1364, 69%) loss:2.1713\n",
      "0m 31s (- 0m 8s) (1559, 79%) loss:2.1706\n",
      "0m 35s (- 0m 4s) (1754, 89%) loss:2.1676\n",
      "0m 39s (- 0m 0s) (1949, 99%) loss:2.1668\n",
      "> train\n",
      "********* evaluation *********\n",
      "avg loss: 2.166196395039315\n",
      "avg ndcg: 0.6907492405973927\n",
      "******************************\n",
      "===== epoch 2 =====\n",
      "0m 4s (- 0m 40s) (194, 9%) loss:2.1522\n",
      "0m 8s (- 0m 33s) (389, 19%) loss:2.1673\n",
      "0m 12s (- 0m 28s) (584, 29%) loss:2.1664\n",
      "0m 16s (- 0m 24s) (779, 39%) loss:2.1617\n",
      "0m 19s (- 0m 20s) (974, 49%) loss:2.1609\n",
      "0m 23s (- 0m 15s) (1169, 59%) loss:2.1635\n",
      "0m 27s (- 0m 11s) (1364, 69%) loss:2.1664\n",
      "0m 31s (- 0m 7s) (1559, 79%) loss:2.1618\n",
      "0m 35s (- 0m 3s) (1754, 89%) loss:2.1623\n",
      "0m 38s (- 0m 0s) (1949, 99%) loss:2.1606\n",
      "> train\n",
      "0m 8s (- 0m 32s) (389, 19%) loss:2.1585\n",
      "0m 11s (- 0m 27s) (584, 29%) loss:2.1585\n",
      "0m 15s (- 0m 23s) (779, 39%) loss:2.1579\n",
      "0m 19s (- 0m 19s) (974, 49%) loss:2.1569\n",
      "0m 23s (- 0m 15s) (1169, 59%) loss:2.1588\n",
      "0m 26s (- 0m 11s) (1364, 69%) loss:2.1551\n",
      "0m 30s (- 0m 7s) (1559, 79%) loss:2.1575\n",
      "0m 34s (- 0m 3s) (1754, 89%) loss:2.1571\n",
      "0m 38s (- 0m 0s) (1949, 99%) loss:2.1587\n",
      "> train\n",
      "********* evaluation *********\n",
      "avg loss: 2.156040638258608\n",
      "avg ndcg: 0.6952405986783055\n",
      "******************************\n",
      "===== epoch 5 =====\n",
      "0m 4s (- 0m 36s) (194, 9%) loss:2.1453\n",
      "0m 8s (- 0m 32s) (389, 19%) loss:2.1532\n",
      "0m 11s (- 0m 27s) (584, 29%) loss:2.1578\n",
      "0m 15s (- 0m 23s) (779, 39%) loss:2.1536\n",
      "0m 19s (- 0m 19s) (974, 49%) loss:2.1606\n",
      "0m 23s (- 0m 15s) (1169, 59%) loss:2.1543\n",
      "0m 26s (- 0m 11s) (1364, 69%) loss:2.1579\n",
      "0m 30s (- 0m 7s) (1559, 79%) loss:2.1537\n",
      "0m 34s (- 0m 3s) (1754, 89%) loss:2.1561\n",
      "0m 38s (- 0m 0s) (1949, 99%) loss:2.1581\n",
      "> train\n",
      "********* evaluation *********\n",
      "avg loss: 2.1548352924105094\n",
      "avg ndcg: 0.6955866571074303\n",
      "******************************\n",
      "===== epoch 6 =====\n",
      "0m 4s (- 0m 37s) (194, 9%) loss:2.1444\n",
      "0m 7s (- 0m 31s) (389, 19%) loss:2.1553\n",
      "0m 11s (- 0m 27s) (584, 29%) loss:2.1562\n",
      "0m 15s (- 0m 23s) (779, 39%) loss:2.1563\n",
      "0m 19s (- 0m 19s) (974, 49%) loss:2.1538\n",
      "0m 22s (- 0m 15s) (1169, 59%) loss:2.1490\n",
      "0m 26s (- 0m 11s) (1364, 69%) loss:2.1556\n",
      "0m 30s (- 0m 7s) (1559, 79%) loss:2.1566\n",
      "0m 34s (- 0m 3s) (1754, 89%) loss:2.1550\n",
      "0m 38s (- 0m 0s) (1949, 99%) loss:2.1568\n",
      "> train\n",
      "********* evaluation *********\n",
      "avg loss: 2.1537951213938085\n",
      "avg ndcg: 0.6955471622995435\n",
      "******************************\n",
      "===== epoch 7 =====\n",
      "0m 4s (- 0m 37s) (194, 9%) loss:2.1418\n",
      "0m 7s (- 0m 31s) (389, 19%) loss:2.1533\n",
      "0m 11s (- 0m 27s) (584, 29%) loss:2.1569\n",
      "0m 15s (- 0m 23s) (779, 39%) loss:2.1532\n",
      "0m 19s (- 0m 19s) (974, 49%) loss:2.1532\n",
      "0m 23s (- 0m 15s) (1169, 59%) loss:2.1581\n",
      "0m 27s (- 0m 11s) (1364, 69%) loss:2.1538\n",
      "0m 30s (- 0m 7s) (1559, 79%) loss:2.1548\n",
      "0m 34s (- 0m 3s) (1754, 89%) loss:2.1528\n",
      "0m 38s (- 0m 0s) (1949, 99%) loss:2.1518\n",
      "> train\n",
      "********* evaluation *********\n",
      "avg loss: 2.152740001678467\n",
      "avg ndcg: 0.6954046510932768\n",
      "******************************\n",
      "===== epoch 8 =====\n",
      "0m 3s (- 0m 36s) (194, 9%) loss:2.1385\n",
      "0m 7s (- 0m 31s) (389, 19%) loss:2.1546\n",
      "0m 11s (- 0m 27s) (584, 29%) loss:2.1490\n",
      "0m 15s (- 0m 23s) (779, 39%) loss:2.1546\n",
      "0m 19s (- 0m 19s) (974, 49%) loss:2.1546\n",
      "0m 22s (- 0m 15s) (1169, 59%) loss:2.1530\n",
      "0m 26s (- 0m 11s) (1364, 69%) loss:2.1541\n",
      "0m 30s (- 0m 7s) (1559, 79%) loss:2.1543\n",
      "0m 34s (- 0m 3s) (1754, 89%) loss:2.1555\n",
      "0m 38s (- 0m 0s) (1949, 99%) loss:2.1525\n",
      "> train\n",
      "********* evaluation *********\n",
      "avg loss: 2.151730862619443\n",
      "avg ndcg: 0.6964514065436376\n",
      "******************************\n",
      "===== epoch 9 =====\n",
      "0m 3s (- 0m 36s) (194, 9%) loss:2.1441\n",
      "0m 7s (- 0m 31s) (389, 19%) loss:2.1522\n",
      "0m 11s (- 0m 26s) (584, 29%) loss:2.1497\n",
      "0m 15s (- 0m 22s) (779, 39%) loss:2.1536\n",
      "0m 18s (- 0m 19s) (974, 49%) loss:2.1499\n",
      "0m 22s (- 0m 15s) (1169, 59%) loss:2.1497\n",
      "0m 26s (- 0m 11s) (1364, 69%) loss:2.1541\n",
      "0m 30s (- 0m 7s) (1559, 79%) loss:2.1534\n",
      "0m 34s (- 0m 3s) (1754, 89%) loss:2.1521\n",
      "0m 38s (- 0m 0s) (1949, 99%) loss:2.1536\n",
      "> train\n",
      "********* evaluation *********\n",
      "avg loss: 2.15044931448798\n",
      "avg ndcg: 0.6975735163370476\n",
      "******************************\n",
      "===== epoch 10 =====\n",
      "0m 4s (- 0m 36s) (194, 9%) loss:2.1391\n",
      "0m 7s (- 0m 30s) (389, 19%) loss:2.1562\n",
      "0m 11s (- 0m 26s) (584, 29%) loss:2.1483\n",
      "0m 14s (- 0m 22s) (779, 39%) loss:2.1491\n",
      "0m 18s (- 0m 18s) (974, 49%) loss:2.1510\n",
      "0m 22s (- 0m 15s) (1169, 59%) loss:2.1488\n",
      "0m 26s (- 0m 11s) (1364, 69%) loss:2.1529\n",
      "0m 30s (- 0m 7s) (1559, 79%) loss:2.1533\n",
      "0m 33s (- 0m 3s) (1754, 89%) loss:2.1510\n",
      "0m 37s (- 0m 0s) (1949, 99%) loss:2.1531\n",
      "> train\n",
      "********* evaluation *********\n",
      "avg loss: 2.1499208063191917\n",
      "avg ndcg: 0.6976222200842621\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "total_epoch = 10\n",
    "for i in range(total_epoch):\n",
    "    print('===== epoch {} ====='.format(i+1))\n",
    "    train_iter(model, train_dl, valid_dl, [opt], print_ratio=0.1)\n",
    "    torch.save(model.state_dict(), f'./model_ckpt/abs_neg_cnt_celoss_yrenorm_all_data_e{i}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_dl.dataset.change_state('valid', shuffle=False)\n",
    "#evaluation(model, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), f'./model_ckpt/second_try_fold{curr_fold}_e{total_epoch}.ckpt')\n",
    "#torch.save(model.state_dict(), f'./model_ckpt/gru2_abs_neg_cnt_all_data_e{total_epoch}.ckpt')\n",
    "#torch.save(model.state_dict(), f'./model_ckpt/abs_neg_cnt_celoss_yrenorm_all_data_e{total_epoch}.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl.dataset.change_state('test', shuffle=False)\n",
    "load_epoch = 5 - 1\n",
    "model.load_state_dict(torch.load(f'./model_ckpt/abs_neg_cnt_celoss_yrenorm_all_data_e{load_epoch}.ckpt', map_location=avail_device))\n",
    "model.eval()\n",
    "topk = 3\n",
    "\n",
    "all_predict = []\n",
    "with torch.no_grad():\n",
    "    for batch_iter, (ids, xs, cs, ys) in enumerate(test_dl):\n",
    "        px, = model.data_preprocess(xs)\n",
    "        #px, pc = model.data_preprocess(xs, c=cs)\n",
    "        #px_amt_c = (px[:, :, :49].unsqueeze(-1) * pc).reshape(-1, 49, pc.shape[-1]).permute(2, 0, 1)\n",
    "        #px_cnt_c = (px[:, :, 49:].unsqueeze(-1) * pc).reshape(-1, 49, pc.shape[-1]).permute(2, 0, 1)\n",
    "        \n",
    "        #fake_batch = px.shape[0]*px.shape[1]\n",
    "        #_, px_amt_mix = model.card_amt_attn(model.amt_cls_token(torch.zeros(fake_batch, ).long().to(avail_device)), px_amt_c, px_amt_c)\n",
    "        #_, px_cnt_mix = model.card_cnt_attn(model.cnt_cls_token(torch.zeros(fake_batch, ).long().to(avail_device)), px_cnt_c, px_cnt_c)\n",
    "        \n",
    "        #px_mix = torch.cat((px_amt_mix.reshape(px.shape[0], px.shape[1], 49), \n",
    "        #                    px_cnt_mix.reshape(px.shape[0], px.shape[1], 49)), dim=-1)\n",
    "        \n",
    "        rnn_output, _ = model.rnn_layer(px)\n",
    "        #rnn_output, _ = model.rnn_layer(px_mix)\n",
    "        linear_output = model.linear_layer(rnn_output[:, -1, :])[:, model.real_item]\n",
    "        topk_index = linear_output.argsort(dim=-1, descending=True)[:, :topk]\n",
    "        topk_shop_tag = model.real_item[topk_index.cpu()] + 1\n",
    "        batch_predict = torch.cat((ids.long().unsqueeze(-1), topk_shop_tag), dim=-1).tolist()\n",
    "        all_predict.extend(batch_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_predict, columns=['chid', 'top1', 'top2', 'top3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(f\"./submits/basic_brc_fold{curr_fold}_e{total_epoch}.csv\", index=False)\n",
    "#df.to_csv(f\"./submits/gru2_abs_neg_cnt_all_data_e{total_epoch}.csv\", index=False)\n",
    "df.to_csv(f\"./submits/abs_neg_cnt_celoss_yrenorm_all_data_e{load_epoch}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [f'./model_ckpt/transaction_with_user_emb_fold{i}_e10.ckpt' for i in range(5)]\n",
    "ensemble_models = []\n",
    "\n",
    "for name in model_names:\n",
    "    curr_model = RNNRecommend(hidden_dim=512, num_layers=1, device=avail_device).to(avail_device)\n",
    "    curr_model.load_state_dict(torch.load(name, map_location=avail_device))\n",
    "    curr_model.eval()\n",
    "    ensemble_models.append(curr_model)\n",
    "    \n",
    "test_dl.dataset.test()\n",
    "topk = 3\n",
    "\n",
    "ensemble_predict = []\n",
    "with torch.no_grad():\n",
    "    for batch_iter, (ids, xs, es, ys) in enumerate(test_dl):\n",
    "        \n",
    "        model_outs = []\n",
    "        for curr_model in ensemble_models:\n",
    "            px = curr_model.data_preprocess(xs)[0]\n",
    "            pe = curr_model.user_embedding(curr_model.user_encoding(es).reshape(es.shape[0], es.shape[1], -1))\n",
    "            px = torch.cat((px, pe), dim=-1)\n",
    "            rnn_output, _ = curr_model.rnn_layer(px)\n",
    "            linear_output = model.linear_layer(rnn_output[:, -1, :])[:, model.real_item].detach()\n",
    "            model_outs.append(linear_output)\n",
    "            \n",
    "        model_outs = sum(model_outs)/len(ensemble_models)\n",
    "        topk_index = model_outs.argsort(dim=-1, descending=True)[:, :topk]\n",
    "        topk_shop_tag = curr_model.real_item[topk_index.cpu()] + 1\n",
    "        batch_predict = torch.cat((ids.long().unsqueeze(-1), topk_shop_tag), dim=-1).tolist()\n",
    "        ensemble_predict.extend(batch_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"./submits/transaction_with_user_emb_fold_kfe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
